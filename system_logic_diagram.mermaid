graph TD
    %% Data Input Layer
    A[📁 Raw Data Upload<br/>CSV/Parquet Files] --> B[🔍 Data Ingestion<br/>DataLoader]
    
    %% Preprocessing Layer
    B --> C[⚙️ Preprocessing Pipeline]
    C --> C1[📝 Column Standardization<br/>lowercase, no spaces]
    C --> C2[🎯 Auto Type Detection<br/>datetime, numeric, categorical]
    C --> C3[🏷️ Metadata Addition<br/>row_id, timestamp]
    
    C1 --> D[🛠️ Advanced Feature Engineering]
    C2 --> D
    C3 --> D
    
    %% Feature Engineering Layer
    D --> D1[📊 Numerical Features<br/>raw, log, sqrt, percentile]
    D --> D2[📅 Temporal Features<br/>hour, day, month, weekend]
    D --> D3[🏷️ Categorical Features<br/>frequency, target encoding]
    D --> D4[📝 Text Features<br/>length, word count, chars]
    D --> D5[🔗 Interaction Features<br/>ratios, products]
    
    %% Algorithm Ensemble Layer
    D1 --> E[🧠 7-Algorithm Ensemble]
    D2 --> E
    D3 --> E
    D4 --> E
    D5 --> E
    
    %% Individual Algorithms
    E --> F1[🌳 Isolation Forest<br/>Tree-based isolation<br/>Weight: 15%]
    E --> F2[🔍 Local Outlier Factor<br/>Density-based detection<br/>Weight: 20%]
    E --> F3[🎯 One-Class SVM<br/>Boundary detection<br/>Weight: 15%]
    E --> F4[🧠 Deep Autoencoder<br/>Neural reconstruction<br/>Weight: 25%]
    E --> F5[📊 Statistical Outlier<br/>Z-score & IQR<br/>Weight: 10%]
    E --> F6[🔘 Density Clustering<br/>DBSCAN noise detection<br/>Weight: 10%]
    E --> F7[⏰ Temporal Anomaly<br/>Time-series patterns<br/>Weight: 5%]
    
    %% Algorithm Processing
    F1 --> G1[📈 Anomaly Scores<br/>Probability: 0-1]
    F2 --> G2[📈 Anomaly Scores<br/>Probability: 0-1]
    F3 --> G3[📈 Anomaly Scores<br/>Probability: 0-1]
    F4 --> G4[📈 Anomaly Scores<br/>Probability: 0-1]
    F5 --> G5[📈 Anomaly Scores<br/>Probability: 0-1]
    F6 --> G6[📈 Anomaly Scores<br/>Probability: 0-1]
    F7 --> G7[📈 Anomaly Scores<br/>Probability: 0-1]
    
    %% Ensemble Voting
    G1 --> H[🗳️ Weighted Ensemble Voting]
    G2 --> H
    G3 --> H
    G4 --> H
    G5 --> H
    G6 --> H
    G7 --> H
    
    %% Consensus Decision
    H --> I{🤔 Consensus Check<br/>Multiple algorithms agree?}
    I -->|High Confidence<br/>Score > 0.8| J1[🚨 High Priority<br/>Immediate Review]
    I -->|Medium Confidence<br/>Score > 0.6| J2[⚠️ Medium Priority<br/>Review Soon]
    I -->|Low Confidence<br/>Score > 0.3| J3[ℹ️ Low Priority<br/>Monitor Pattern]
    I -->|No Consensus<br/>Score < 0.3| J4[✅ Normal Record<br/>No Action]
    
    %% Explainability Layer
    J1 --> K[🔍 Explainability Engine]
    J2 --> K
    J3 --> K
    
    K --> K1[📋 Generate Explanations<br/>Why was it flagged?]
    K --> K2[🎯 Feature Contributions<br/>Which fields caused it?]
    K --> K3[💡 Suggested Actions<br/>What should you do?]
    
    %% Human Review Interface
    K1 --> L[👤 Human Review Interface]
    K2 --> L
    K3 --> L
    
    L --> M{👨‍💼 Human Decision}
    M -->|✅ Confirm Anomaly| N1[✅ True Positive<br/>Reward Algorithms]
    M -->|❌ Dismiss Flag| N2[❌ False Positive<br/>Penalize Algorithms]
    M -->|🚨 Escalate Issue| N3[🚨 Critical Issue<br/>Immediate Action]
    
    %% Adaptive Learning Loop
    N1 --> O[🧠 Adaptive Learning Engine]
    N2 --> O
    N3 --> O
    
    O --> O1[📊 Update Algorithm Performance<br/>Track success rates]
    O --> O2[⚖️ Adjust Algorithm Weights<br/>Better algorithms get more influence]
    O --> O3[🎛️ Tune Detection Thresholds<br/>Balance precision vs recall]
    
    %% Weight Update Feedback
    O1 --> P{📈 Enough Feedback<br/>≥ 10 samples?}
    P -->|Yes| Q[🔄 Recalculate Weights<br/>Exponential moving average]
    P -->|No| R[⏳ Keep Learning<br/>Need more data]
    
    Q --> Q1[📊 Normalize Weights<br/>Ensure sum = 1.0]
    Q1 --> S[💾 Update Ensemble<br/>Apply new weights]
    
    %% Audit and Governance
    S --> T[📝 Audit Trail]
    L --> T
    O --> T
    
    T --> T1[🗄️ Database Storage<br/>All decisions tracked]
    T --> T2[📊 Performance Metrics<br/>Precision, Recall, F1]
    T --> T3[📈 Trend Analysis<br/>Quality over time]
    
    %% Dashboard and Monitoring
    T1 --> U[📊 Dashboard Display]
    T2 --> U
    T3 --> U
    
    U --> U1[📈 Algorithm Weights<br/>Real-time visualization]
    U --> U2[📊 System Performance<br/>Accuracy metrics]
    U --> U3[📉 Quality Trends<br/>Historical analysis]
    
    %% Continuous Improvement Loop
    S --> V[🔄 Next Data Processing<br/>Improved Detection]
    V --> E
    
    %% Drift Detection (Advanced Feature)
    B --> W[📡 Drift Detector<br/>Monitor data changes]
    W --> W1[📊 Statistical Tests<br/>PSI, KS-test]
    W --> W2[⚠️ Alert on Drift<br/>Retrain if needed]
    W2 --> O
    
    %% Styling
    classDef inputNode fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef processNode fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef algorithmNode fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef decisionNode fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef humanNode fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef learningNode fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef storageNode fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    
    class A,B inputNode
    class C,C1,C2,C3,D,D1,D2,D3,D4,D5 processNode
    class F1,F2,F3,F4,F5,F6,F7,G1,G2,G3,G4,G5,G6,G7 algorithmNode
    class I,M,P decisionNode
    class L,N1,N2,N3 humanNode
    class O,O1,O2,O3,Q,Q1,S,V learningNode
    class T,T1,T2,T3,U,U1,U2,U3 storageNode