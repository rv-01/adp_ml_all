graph TD
    %% Data Input Layer
    A[ğŸ“ Raw Data Upload<br/>CSV/Parquet Files] --> B[ğŸ” Data Ingestion<br/>DataLoader]
    
    %% Preprocessing Layer
    B --> C[âš™ï¸ Preprocessing Pipeline]
    C --> C1[ğŸ“ Column Standardization<br/>lowercase, no spaces]
    C --> C2[ğŸ¯ Auto Type Detection<br/>datetime, numeric, categorical]
    C --> C3[ğŸ·ï¸ Metadata Addition<br/>row_id, timestamp]
    
    C1 --> D[ğŸ› ï¸ Advanced Feature Engineering]
    C2 --> D
    C3 --> D
    
    %% Feature Engineering Layer
    D --> D1[ğŸ“Š Numerical Features<br/>raw, log, sqrt, percentile]
    D --> D2[ğŸ“… Temporal Features<br/>hour, day, month, weekend]
    D --> D3[ğŸ·ï¸ Categorical Features<br/>frequency, target encoding]
    D --> D4[ğŸ“ Text Features<br/>length, word count, chars]
    D --> D5[ğŸ”— Interaction Features<br/>ratios, products]
    
    %% Algorithm Ensemble Layer
    D1 --> E[ğŸ§  7-Algorithm Ensemble]
    D2 --> E
    D3 --> E
    D4 --> E
    D5 --> E
    
    %% Individual Algorithms
    E --> F1[ğŸŒ³ Isolation Forest<br/>Tree-based isolation<br/>Weight: 15%]
    E --> F2[ğŸ” Local Outlier Factor<br/>Density-based detection<br/>Weight: 20%]
    E --> F3[ğŸ¯ One-Class SVM<br/>Boundary detection<br/>Weight: 15%]
    E --> F4[ğŸ§  Deep Autoencoder<br/>Neural reconstruction<br/>Weight: 25%]
    E --> F5[ğŸ“Š Statistical Outlier<br/>Z-score & IQR<br/>Weight: 10%]
    E --> F6[ğŸ”˜ Density Clustering<br/>DBSCAN noise detection<br/>Weight: 10%]
    E --> F7[â° Temporal Anomaly<br/>Time-series patterns<br/>Weight: 5%]
    
    %% Algorithm Processing
    F1 --> G1[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F2 --> G2[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F3 --> G3[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F4 --> G4[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F5 --> G5[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F6 --> G6[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    F7 --> G7[ğŸ“ˆ Anomaly Scores<br/>Probability: 0-1]
    
    %% Ensemble Voting
    G1 --> H[ğŸ—³ï¸ Weighted Ensemble Voting]
    G2 --> H
    G3 --> H
    G4 --> H
    G5 --> H
    G6 --> H
    G7 --> H
    
    %% Consensus Decision
    H --> I{ğŸ¤” Consensus Check<br/>Multiple algorithms agree?}
    I -->|High Confidence<br/>Score > 0.8| J1[ğŸš¨ High Priority<br/>Immediate Review]
    I -->|Medium Confidence<br/>Score > 0.6| J2[âš ï¸ Medium Priority<br/>Review Soon]
    I -->|Low Confidence<br/>Score > 0.3| J3[â„¹ï¸ Low Priority<br/>Monitor Pattern]
    I -->|No Consensus<br/>Score < 0.3| J4[âœ… Normal Record<br/>No Action]
    
    %% Explainability Layer
    J1 --> K[ğŸ” Explainability Engine]
    J2 --> K
    J3 --> K
    
    K --> K1[ğŸ“‹ Generate Explanations<br/>Why was it flagged?]
    K --> K2[ğŸ¯ Feature Contributions<br/>Which fields caused it?]
    K --> K3[ğŸ’¡ Suggested Actions<br/>What should you do?]
    
    %% Human Review Interface
    K1 --> L[ğŸ‘¤ Human Review Interface]
    K2 --> L
    K3 --> L
    
    L --> M{ğŸ‘¨â€ğŸ’¼ Human Decision}
    M -->|âœ… Confirm Anomaly| N1[âœ… True Positive<br/>Reward Algorithms]
    M -->|âŒ Dismiss Flag| N2[âŒ False Positive<br/>Penalize Algorithms]
    M -->|ğŸš¨ Escalate Issue| N3[ğŸš¨ Critical Issue<br/>Immediate Action]
    
    %% Adaptive Learning Loop
    N1 --> O[ğŸ§  Adaptive Learning Engine]
    N2 --> O
    N3 --> O
    
    O --> O1[ğŸ“Š Update Algorithm Performance<br/>Track success rates]
    O --> O2[âš–ï¸ Adjust Algorithm Weights<br/>Better algorithms get more influence]
    O --> O3[ğŸ›ï¸ Tune Detection Thresholds<br/>Balance precision vs recall]
    
    %% Weight Update Feedback
    O1 --> P{ğŸ“ˆ Enough Feedback<br/>â‰¥ 10 samples?}
    P -->|Yes| Q[ğŸ”„ Recalculate Weights<br/>Exponential moving average]
    P -->|No| R[â³ Keep Learning<br/>Need more data]
    
    Q --> Q1[ğŸ“Š Normalize Weights<br/>Ensure sum = 1.0]
    Q1 --> S[ğŸ’¾ Update Ensemble<br/>Apply new weights]
    
    %% Audit and Governance
    S --> T[ğŸ“ Audit Trail]
    L --> T
    O --> T
    
    T --> T1[ğŸ—„ï¸ Database Storage<br/>All decisions tracked]
    T --> T2[ğŸ“Š Performance Metrics<br/>Precision, Recall, F1]
    T --> T3[ğŸ“ˆ Trend Analysis<br/>Quality over time]
    
    %% Dashboard and Monitoring
    T1 --> U[ğŸ“Š Dashboard Display]
    T2 --> U
    T3 --> U
    
    U --> U1[ğŸ“ˆ Algorithm Weights<br/>Real-time visualization]
    U --> U2[ğŸ“Š System Performance<br/>Accuracy metrics]
    U --> U3[ğŸ“‰ Quality Trends<br/>Historical analysis]
    
    %% Continuous Improvement Loop
    S --> V[ğŸ”„ Next Data Processing<br/>Improved Detection]
    V --> E
    
    %% Drift Detection (Advanced Feature)
    B --> W[ğŸ“¡ Drift Detector<br/>Monitor data changes]
    W --> W1[ğŸ“Š Statistical Tests<br/>PSI, KS-test]
    W --> W2[âš ï¸ Alert on Drift<br/>Retrain if needed]
    W2 --> O
    
    %% Styling
    classDef inputNode fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef processNode fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef algorithmNode fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef decisionNode fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef humanNode fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef learningNode fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef storageNode fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    
    class A,B inputNode
    class C,C1,C2,C3,D,D1,D2,D3,D4,D5 processNode
    class F1,F2,F3,F4,F5,F6,F7,G1,G2,G3,G4,G5,G6,G7 algorithmNode
    class I,M,P decisionNode
    class L,N1,N2,N3 humanNode
    class O,O1,O2,O3,Q,Q1,S,V learningNode
    class T,T1,T2,T3,U,U1,U2,U3 storageNode